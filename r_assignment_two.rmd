---
title: "R Assignment Two"
author: "Katherine Wolf"
date: "February 11, 2020"
output:
    pdf_document:
       includes:
         in_header: preamble-latex.tex
    html_document:
       includes:
         before_body: preamble-mathjax.tex
    latex_engine: xelatex
mainfont: Calibri
monofont: Lucida Console
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE, 
                      warning = FALSE, 
                      message = FALSE)

```


```{r load required packages and read data}

library(foreign)
library(boot)
load("frmgham_recoded.wide.Rdata")

```


```{r}

# Make 2nd category (BMI 18.5-24.9, ideal weight) the referent group:
frmgham_recoded.wide$bmi_cat <- 
  relevel(as.factor(frmgham_recoded.wide$bmi_cat),"2")

```


```{r}

standardized.measures <- function(dataset, index){
  # Create resampled version of dataset using index vector:
  dataset.resampled <- dataset[index,];

  # Fit logistic regression from the resampled dataset:
  # Outcome: hyperten
  # Covariates: bmi_cat, cursmoke, age, sex, educ
  # Store results in object called logistic.frmgham
  
  logistic.frmgham <- ##### COMPLETE ON OWN 
    glm(hyperten ~ bmi_cat + cursmoke + age + sex + educ,
        data = dataset.resampled, 
        family = binomial(link = "logit"))
  
  ##### end complete on own
  
  ### To calculate the measures of association:

  # STEP 1: Create two new versions of the *original* dataset, called
  # frmgham_recoded.wide.obese and frmgham_recoded.wide.ideal
  frmgham_recoded.wide.obese <- 
    frmgham_recoded.wide.ideal <- 
    frmgham_recoded.wide

  # Set BMI to obese (category 4) in population 1 [exposed]
  # and ideal weight (category 2) in population 0 [unexposed]:
  frmgham_recoded.wide.obese$bmi_cat <- "4" # population w/ all obese
  frmgham_recoded.wide.ideal$bmi_cat <- "2" # population w/ all ideal weight

  # STEP 2: Obtain predicted individual risk of hypertension under each new dataset:
  rhat.obese <- predict(logistic.frmgham, type = "response",
                        newdata = frmgham_recoded.wide.obese)
  rhat.ideal <- predict(logistic.frmgham, type = "response",
                        newdata=frmgham_recoded.wide.ideal)

  # STEP 3: Calculate the average risk (proportion) of hypertension
  # in each hypothetical population:
  risk.obese <- mean(rhat.obese)
  risk.ideal <- mean(rhat.ideal)
  
  # Calculate risk difference and risk ratio using ideal weight as reference:
  rd <- risk.obese - risk.ideal
  rr <- risk.obese/risk.ideal

  # STEP 4: Return these estimates:
  return(c(rd, rr))
}

```




```{r}

n.frmgham <- nrow(frmgham_recoded.wide)
stdized.measures <- standardized.measures(frmgham_recoded.wide, 
                                          index=seq(1:n.frmgham))
stdized.rd <- stdized.measures[1] # Risk difference is 1st element
stdized.rr <- stdized.measures[2] # Risk ratio is 2nd element

```



```{r}

set.seed(123)
# Put the bootstrapped sample results into object called bs.standardized
bs.standardized <- boot(frmgham_recoded.wide, standardized.measures, 5000)

if(!exists("ci.rd.bca")) {
  ci.rd.bca <- boot.ci(bs.standardized, type= "bca", index=1)
}

if(!exists("ci.rr.bca")) {
  ci.rr.bca <- boot.ci(bs.standardized, type= "bca", index=2)
}

```


```{r}

rd.samples <- bs.standardized$t[,1]  # risk difference
rr.samples <- bs.standardized$t[,2]  # risk ratio

```

```{r on my own pretending}

# Calculate the standard deviation of each of the series of bootstrapped samples obtained in the above code chunk (use the log-scale for the relative measure). Calculate these centered on the original point estimate (not the sample means).

sd.rd.boot <- sd(rd.samples - stdized.rd)

sd.log.rr.boot <- sd(log(rr.samples) - log(stdized.rr))


# Use these estimates of standard deviation to calculate 95 confidence intervals for the RD and RR using a Normal approximation approach (see slide 12 from lecture notes). (Hint: make sure to do the CI calculation for the RR on the log scale and transform back to ratio scale.)

rd.ci.norm.apx <- c(stdized.rd - 1.96*sd.rd.boot, 
                    stdized.rd + 1.96*sd.rd.boot)

log.rr.ci.norm.apx <- c(log(stdized.rr) - 1.96*sd.log.rr.boot, 
                        log(stdized.rr) + 1.96*sd.log.rr.boot)

rr.ci.norm.apx <- exp(log.rr.ci.norm.apx)

```

# Question One

## State the point estimate and 95% confidence interval for the estimated risk difference and risk ratio for this analysis (20 points)

The estimated standardized risk (cumulative incidence) difference, comparing the hypothetical cumulative incidence of hypertension over the study period in a population in which everyone is obese to that in a population in which everyone is "ideal" weight, is `r round(stdized.rd, 3)`, and the 95% confidence interval is (`r round(ci.rd.bca$bca[4], 3)`, `r round(ci.rd.bca$bca[5], 3)`), where the confidence interval is calculated around the original standardized point estimate for the cumulative incidence difference using the bias-corrected and accelerated method via the `boot` package.

The estimated standardized risk (cumulative incidence) ratio, comparing the hypothetical cumulative incidence of hypertension over the study period in a population in which everyone is obese to that in a population in which everyone is "ideal" weight, is `r round(stdized.rr, 3)`, and the 95% confidence interval is (`r round(ci.rr.bca$bca[4], 3)`, `r round(ci.rr.bca$bca[5], 3)`), where the confidence interval is calculated around the original standardized point estimate for the cumulative incidence ratio using the bias-corrected and accelerated method via the `boot` package.

# Question Two

## Turn in the density plots for the risk difference and risk ratio measures. Briefly describe the pattern that you see. Does the pattern seem to indicate that the sampling distribution of the RD and RR are approximately normal? (10 points)

```{r}

par(mfrow=c(2,1))
plot(density(rd.samples), main = "Bootstrapped Samples of Risk Difference")
plot(density(rr.samples), main = "Bootstrapped Samples of Risk Ratio")
plot(density(log(rr.samples)), main = "Bootstrapped Samples of Log Risk Ratio")
par(mfrow=c(1,1))

```

The density plots for the bootstrapped samples of the risk difference and the risk ratio be looking normal, all right!  Cute little symmetrical hills centered on the point estimates.  The sampling distributions of both the risk difference and risk ratio both look approximately normal, and we can likely use normal approximations to calculate 95% confidence intervals.

\pagebreak

# Question Three

```{r back-of-the-envelope}

# risk difference comparisons

# true value 0.234

# bca ci 0.169, 0.292

# normal ci 0.172, 0.297

# bca middle
(0.169 + 0.292)/2

# bca size
0.292 - 0.169

# normal ci middle
(0.172 + 0.297)/2

# normal size
0.297 - 0.172


# risk ratio comparisons

# true value 1.388

# bca ci 1.273, 1.497

# normal ci 1.279, 1.506

# bca middle
(1.273 + 1.497)/2
# bca size
1.497 - 1.273

# normal ci middle
(1.279 + 1.506)/2

# normal ci size
1.506 - 1.279



```


## Showing your work, calculate the 95% confidence interval for the risk difference using a normal approximation, and report your answer. (Show the formula you used, and the specific values that went into your calculation.) What assumptions does this require? How does this compare to the results from question one? (10 points)

To calculate the normal approximation, I used the formula

$$ \hat{\theta} \pm 1.96*SE_{\hat{\theta}_i} $$



The 95% confidence interval for the risk difference using a normal approximation is (`r round(rd.ci.norm.apx[1], 3)`, `r round(rd.ci.norm.apx[2], 3)`), compared to the bias-corrected and accelerated confidence interval of .

# Question Four

## Showing your work, calculate the 95% confidence interval for the risk ratio, and report your answer. (Show the formula you used, and the specific values that went into your calculation.) What assumptions does this require? How does this compare to the results from question one? (10 points)

To simplify notation, let the point estimate of the cumulative incidence ratio derived via model-based standardization, $\hat{RR}$, be denoted by $\hat{\theta}$, and let each estimate of the cumulative incidence ratio derived from a bootstrap sample of the original data from be denoted by $\hat{\theta}_i$ .

To calculate the 95% confidence interval using a normal approximation, I first calculated the mean of the differences between the model-standardized point estimate of the cumulative incidence ratio and bootstrapped point estimates of the cumulative incidence ratio:

$$
\overline{\Delta_{ln(\hat{\theta_i})}} = \frac{\sum\limits_{i = 1}^{n}(\ln(\hat{\theta}_i )-\ln(\hat{\theta}))}{n}
$$

I then used that mean to calculate the standard deviation of the differences between the log bootstrap sample CIR estimates and the log model-standardized CIR estimate to estimate the standard error of those differences: 

$$
SE_{\ln(\hat{\theta}_i)} 
\approx 
\sqrt{\frac{1}{n-1} \sum\limits_{i = 1}^{n}\left(\left(\ln(\hat{\theta}_i )-\ln(\hat{\theta})\right) - \overline{\Delta_{ln(\hat{\theta_i})}}\right)^2}
$$

(To be honest, I did all of the above steps using the simple R code 
`sd.log.rr.boot <- sd(log(rr.samples) - log(stdized.rr))`.)

I then calculated 

ASSUMPTIONS!!!

This tactic for estimating the confidence interval assumes:

* That the sampling distribution (here, the distribution of the estimates of the log cumulative incidence ratio calculated from the bootstrap samples from the original dataset) be normal, i.e., symmetrical and bell-shaped

* That the sample size is large enough.

$$ e^{\ln(\hat{\theta}) \pm 1.96*SE_{\ln(\hat{\theta}_i)}}, $$

where $SE_{\ln(\hat{\theta}_i)}$ is calculated as

$$ sd(\ln(\hat{\theta}_i) - \ln(\hat{\theta})), $$

and where that is calculated as

mean of differences between model-standardized point estimate of the cumulative incidence ratio and bootstrapped point estimates of the cumulative incidence ratio:


The 95% confidence interval for the risk difference using a normal approximation is `r round(rr.ci.norm.apx[1], 3)`, `r round(rr.ci.norm.apx[2], 3)`, compared to the bias-corrected and accelerated confidence interval of .

Large original sample size (original dataset has `r nrow(frmgham_recoded.wide)` observations).

\pagebreak

# My Questions

## Correct numbers?

## Is this a CID/CIR?  How long was the study period?

## More on question two?

## does prettiness of graph matter

## if we're graphing the risk ratio to show normality, why are we doing calculations on the log of it?

## assumptions for normal approximation

# Appendix: R code

```{r ref.label=knitr::all_labels(), echo = T, eval = F}

```

